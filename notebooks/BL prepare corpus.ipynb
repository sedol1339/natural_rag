{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb36ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4 markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22eb993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 1: prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa15059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import hashlib\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def persistent_hash(text: str):\n",
    "    \"\"\"Generates a persistent SHA-256 hash for a given string.\"\"\"\n",
    "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "def get_page(url: str, cache_dir: Path, delay: float = 0) -> BeautifulSoup:\n",
    "    print('Retrieving', url)\n",
    "    cache_path = cache_dir / f'{persistent_hash(url)}.html'\n",
    "    if cache_path.exists():\n",
    "        html = cache_path.read_text()\n",
    "    else:\n",
    "        if delay:\n",
    "            time.sleep(delay)\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html = response.text\n",
    "        cache_path.write_text(html)\n",
    "    return BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3dd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages\n",
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages?from=Button+Bush+Flowers\n",
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages?from=Filter\n",
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages?from=Marsh\n",
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages?from=Release+3.6.1\n",
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages?from=Sushi+Green+Dye+%28fluid%29\n",
      "Retrieving https://the-betweenlands.fandom.com/wiki/Special:AllPages?from=Winding+Walkways\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = Path('tmp/requests_cache')\n",
    "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# getting index pages\n",
    "\n",
    "DOMAIN = 'https://the-betweenlands.fandom.com'\n",
    "nav_page_urls = [DOMAIN + '/wiki/Special:AllPages']\n",
    "nav_pages = [get_page(nav_page_urls[0], CACHE_DIR)]\n",
    "\n",
    "while True:\n",
    "    nav_block = nav_pages[-1].select_one('.mw-allpages-nav')\n",
    "    assert nav_block\n",
    "    nav_link = nav_block.find_all('a')[-1]\n",
    "    if not 'Next page' in nav_link.text:\n",
    "        break\n",
    "    next_url = DOMAIN + str(nav_link['href'])\n",
    "    nav_page_urls.append(next_url)\n",
    "    new_page = get_page(next_url, CACHE_DIR)\n",
    "    nav_pages.append(new_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a76e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 745 pages\n"
     ]
    }
   ],
   "source": [
    "wiki_page_urls: list[str] = []\n",
    "for page in nav_pages:\n",
    "    content_block = page.select_one('.mw-allpages-body')\n",
    "    assert content_block\n",
    "    for el in content_block.find_all('li'):\n",
    "        if 'allpagesredirect' not in el.get('class', []): # type: ignore\n",
    "            wiki_page_urls.append(DOMAIN + el.find('a')['href']) # type: ignore\n",
    "\n",
    "wiki_page_urls = list(set(wiki_page_urls))\n",
    "\n",
    "print(f'Found {len(wiki_page_urls)} pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef95d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    get_page(page_url, CACHE_DIR, delay=0.5)\n",
    "    for page_url in tqdm(wiki_page_urls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify as md\n",
    "\n",
    "removed_tables: list[str] = []\n",
    "\n",
    "MD_DIR = Path('datasets/bl/docs')\n",
    "MD_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "HTML_DIR = Path('datasets/bl/sources')\n",
    "HTML_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def verbose_table(el: Tag) -> str:\n",
    "    content = el.find('tbody').text.strip()\n",
    "    content = content.split('\\n', 1)[0]\n",
    "    return content\n",
    "\n",
    "def remove_table(el: Tag):\n",
    "    global removed_tables\n",
    "    removed_tables.append(tabstr := verbose_table(el))\n",
    "    print(f'Removing table \"{tabstr}\"')\n",
    "    el.decompose()\n",
    "    \n",
    "def parse_page_for_rag(soup: BeautifulSoup) -> tuple[str, str]:\n",
    "    soup = copy.deepcopy(soup)\n",
    "    title = soup.select_one('#firstHeading').text.strip()\n",
    "    print(f'{title=}')\n",
    "    body = soup.select_one('#mw-content-text').select_one('.mw-content-ltr')\n",
    "    assert body\n",
    "    \n",
    "    # clarifying aside blocks (add \"[Aside block]\" to title)\n",
    "    for aside in body.find_all('aside'):\n",
    "        for h2 in aside.find_all('h2'):\n",
    "            h2.string = f'[Aside block] {h2.text}'\n",
    "    \n",
    "    # removing footer tables\n",
    "    history_block = body.select_one('#History')\n",
    "    if history_block is None:\n",
    "        # remove all trailing tables (usually in changelog pages)\n",
    "        children = [c for c in body.children if isinstance(c, Tag)]\n",
    "        for child in children[::-1]:\n",
    "            if child.name == 'table' and 'margin: auto; width: 85%' in child.get('style', ''):\n",
    "                remove_table(child)\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        # remove tables after \"history\" section\n",
    "        pos = body.index(history_block.parent)\n",
    "        children = [c for c in list(body.children)[pos + 1:] if isinstance(c, Tag)]\n",
    "        for child in children:\n",
    "            if child.name == 'table' and 'margin: auto; width: 85%' in child.get('style', ''):\n",
    "                remove_table(child)\n",
    "           \n",
    "    body_md = md(str(body), strip=['img', 'a'])\n",
    "    \n",
    "    # fix multiple \\n in a row\n",
    "    body_md = re.sub(r'\\n[\\n\\s]+\\n', '\\n\\n', body_md)\n",
    "    \n",
    "    # remove trailing brackets \"Farming[]\"\" or \"Leggings: 2 ()  \"\n",
    "    body_md = re.sub(r'\\[\\]\\s*\\n', '\\n', body_md)\n",
    "    body_md = re.sub(r'\\(\\)\\s*\\n', '\\n', body_md)\n",
    "            \n",
    "    return title, body_md\n",
    "\n",
    "doc_titles: list[str] = []\n",
    "for page in pages:\n",
    "    doc_title, doc_content = parse_page_for_rag(page)\n",
    "    \n",
    "    assert doc_title not in doc_titles, f'duplicate title {doc_title}'\n",
    "    doc_titles.append(doc_title)\n",
    "    \n",
    "    Path(MD_DIR / f'{doc_title}.md').write_text(f'# {doc_title}\\n\\n' + doc_content)\n",
    "    Path(HTML_DIR / f'{doc_title}.html').write_text(str(page))\n",
    "\n",
    "print('Removed tables:')\n",
    "from collections import Counter\n",
    "print(Counter(removed_tables))\n",
    "\n",
    "for nav_idx, page in enumerate(nav_pages):\n",
    "    Path(HTML_DIR / f'AllPages_{nav_idx}.html').write_text(str(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca63e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 745\n",
      "Shortest documents: [('Snowfall', 279), ('Release 3.9.1', 280), ('Environment', 299), ('Release 3.9.6', 301)]\n",
      "Longest documents: [('Loot Tables', 73440), ('Block IDs', 45060), ('Item IDs', 42362), ('Infusions', 25090)]\n",
      "Symbols: 2104571\n",
      "Words: 309186\n",
      "Pages (assuming 1800 chars/page): 1169\n",
      "gpt-4o-mini tokens: 571046\n"
     ]
    }
   ],
   "source": [
    "# stage 2: validate QA\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "MD_DIR = Path('datasets/bl/docs')\n",
    "\n",
    "md_files = {\n",
    "    path.stem: path.read_text()\n",
    "    for path in MD_DIR.glob('*.md')\n",
    "}\n",
    "\n",
    "lengths = {\n",
    "    name: len(content)\n",
    "    for name, content in md_files.items()\n",
    "}\n",
    "lengths = dict(sorted(lengths.items(), key=lambda x: x[1]))\n",
    "\n",
    "print('Total documents:', len(md_files))\n",
    "print('Shortest documents:', list(lengths.items())[:4])\n",
    "print('Longest documents:', list(lengths.items())[::-1][:4])\n",
    "\n",
    "joint_text = '\\n\\n'.join(md_files.values())\n",
    "\n",
    "print('Symbols:', len(joint_text))\n",
    "print('Words:', len(re.findall(r'\\w+', joint_text)))\n",
    "print('Pages (assuming 1800 chars/page):', len(joint_text) // 1800)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "model_name = 'gpt-4o-mini'\n",
    "encoding = tiktoken.encoding_for_model(model_name)\n",
    "n_tokens = len(encoding.encode(joint_text))\n",
    "print(f'{model_name} tokens:', n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b396ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA data validation (TODO remove)\n",
    "\n",
    "with open('datasets/bl/qa_v2.yaml', 'r') as file:\n",
    "    qa = yaml.safe_load(file)\n",
    "\n",
    "for sample in qa:\n",
    "    if 'sources' not in sample:\n",
    "        continue\n",
    "    for src in sample['sources']:\n",
    "        \n",
    "        doc = md_files[src['doc']]\n",
    "        doc = doc.replace('*', '')\n",
    "        doc_lines_nohashmark = set([\n",
    "            l.replace('[Aside block]', '').replace('#', '').strip()\n",
    "            for l in doc.split('\\n')\n",
    "        ])\n",
    "        \n",
    "        if 'loc' in src:\n",
    "            locs = src['loc']\n",
    "            if isinstance(locs, str):\n",
    "                locs = [locs]\n",
    "            for loc in locs:\n",
    "                loc = loc.removesuffix('...')\n",
    "                assert loc in doc\n",
    "        \n",
    "        if 'sec' in src:\n",
    "            secs = src['sec']\n",
    "            if isinstance(secs, str):\n",
    "                secs = [secs]\n",
    "            for sec in secs:\n",
    "                assert sec in doc_lines_nohashmark\n",
    "\n",
    "    if 'answer' in sample:\n",
    "        eval = sample['eval']\n",
    "        scores = [x['score'] for x in eval]\n",
    "        positive_scores = [x for x in scores if x > 0]\n",
    "        assert sum(positive_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab540dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_sentences(text: str) -> list[str]:\n",
    "#     \"\"\"Represents text as a list of sentences, keeping all the\n",
    "#     characters, including trailing spaces and linebreaks. A sentence\n",
    "#     is either a line, or a sentence found by sent_tokenize within a\n",
    "#     line. \n",
    "#     \"\"\"\n",
    "#     ...\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# text = \"Hello, world! How are you today? The U.S. government is in session.\"\n",
    "\n",
    "# sentences = sent_tokenize(text)\n",
    "# sentences\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# MD_DIR = Path('datasets/bl/docs')\n",
    "\n",
    "# docs: dict[str, list[str]] = []\n",
    "\n",
    "# for path in MD_DIR.glob('*.md'):\n",
    "#     sentences_or_lines: list[str] = []\n",
    "    \n",
    "#     for line in path.read_text().splitlines(keepends=True):\n",
    "#         sentences_or_lines.append(line)\n",
    "        \n",
    "#     docs[path.stem] = sentences_or_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737442fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to pydantic\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open('datasets/bl/qa_v2.yaml', 'r') as file:\n",
    "    qa = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63545751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from natural_rag.data import Question\n",
    "\n",
    "pydantic_questions: list[Question] = []\n",
    "\n",
    "for sample in qa:\n",
    "\n",
    "    question_as_dict: dict[str, Any] = {'text': sample['question']}\n",
    "\n",
    "    if 'answer' in sample:\n",
    "        question_as_dict['reference_answer'] = sample['answer']\n",
    "\n",
    "    question_as_dict['metadata'] = {'reasoning_type': sample['reasoning type']}\n",
    "    if 'comment' in sample:\n",
    "        question_as_dict['metadata']['comment'] = sample['comment']\n",
    "\n",
    "    if 'eval' in sample:\n",
    "        question_as_dict['eval_rules'] = []\n",
    "        for check_as_dict in sample['eval']:\n",
    "            check_as_dict = check_as_dict.copy()\n",
    "            if 'comment' in check_as_dict:\n",
    "                check_as_dict['metadata'] = {'comment': check_as_dict.pop('comment')}\n",
    "            question_as_dict['eval_rules'].append(check_as_dict) # type: ignore\n",
    "\n",
    "    if 'sources' in sample:\n",
    "        question_as_dict['relevant'] = []\n",
    "        for source_as_dict in sample['sources']:\n",
    "            item = {'doc_id': source_as_dict['doc']}\n",
    "            locs: list[str] = []\n",
    "            if 'loc' in source_as_dict:\n",
    "                loc = source_as_dict['loc']\n",
    "                if isinstance(loc, str):\n",
    "                    loc = [loc]\n",
    "                locs += [x.removesuffix('...') for x in loc]\n",
    "            if 'sec' in source_as_dict:\n",
    "                sec = source_as_dict['sec']\n",
    "                if isinstance(sec, str):\n",
    "                    sec = [sec]\n",
    "                locs += [f'<{x}>' for x in sec]\n",
    "            if locs:\n",
    "                item['loc'] = locs\n",
    "            question_as_dict['relevant'].append(item) # type: ignore\n",
    "\n",
    "    pydantic_questions.append(Question.model_validate(question_as_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db06597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from natural_rag.data import Document\n",
    "\n",
    "pydantic_documents: list[Document] = []\n",
    "\n",
    "HTML_DIR = Path('datasets/bl/sources')\n",
    "\n",
    "for path in sorted(HTML_DIR.glob('*.html')):\n",
    "    doc = {\n",
    "        'id': path.stem,\n",
    "        'title': path.stem,\n",
    "        'source_ext': '.html',\n",
    "    }\n",
    "    pydantic_documents.append(Document.model_validate(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47a1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natural_rag.data import RAGDataset\n",
    "datset = RAGDataset(\n",
    "    documents={doc.id: doc for doc in pydantic_documents},\n",
    "    questions=pydantic_questions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset.dump_to_dir('bl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a873ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
